# Post-training datasets

Post-training datasets have a precise structure with instructions and answers (supervised fine-tuning) or instructions and chosen/rejected answers (preference alignment). Conversational structures are a lot rarer than the raw text used for pre-training, which is why we often need to process seed data and refine it to improve the accuracy, diversity, and complexity of the samples. More information and examples are available in my repo ğŸ’¾ LLM Datasets.

* Storage & chat templates: Because of the conversational structure, post-training datasets are stored in a specific format like ShareGPT or OpenAI/HF. Then, these formats are mapped to a chat template like ChatML or Alpaca to produce the final samples the model is trained on.
* Synthetic data generation: Create instruction-response pairs based on seed data using frontier models like GPT-4o. This approach allows for flexible and scalable dataset creation with high-quality answers. Key considerations include designing diverse seed tasks and effective system prompts.
* Data enhancement: Enhance existing samples using techniques like verified outputs (using unit tests or solvers), multiple answers with rejection sampling, Auto-Evol, Chain-of-Thought, Branch-Solve-Merge, personas, etc.
* Quality filtering: Traditional techniques involve rule-based filtering, removing duplicates or near-duplicates (with MinHash or embeddings), and n-gram decontamination. Reward models and judge LLMs complement this step with fine-grained and customizable quality control.



**ç®€å•è§£é‡Šï¼š**

åè®­ç»ƒï¼ˆpost-trainingï¼‰çš„æ•°æ®é›†æœ‰ä¸¤ç§å¸¸è§æ ¼å¼ï¼š

* **SFTï¼ˆSupervised Fine-Tuningï¼‰æ•°æ®é›†ï¼š** ä¸€ä¸ªâ€œæŒ‡ä»¤-ç­”æ¡ˆâ€å¯¹ã€‚æ¯”å¦‚ï¼Œâ€œè¯·å†™ä¸€é¦–è¯—â€ï¼Œæ¨¡å‹ç»™å‡ºç­”æ¡ˆã€‚
* **åå¥½æ•°æ®é›†ï¼ˆPreference Alignmentï¼Œå¦‚RLHFï¼‰ï¼š** ä¸€ä¸ªæŒ‡ä»¤+å¤šä¸ªç­”æ¡ˆï¼Œæ ‡å‡ºå“ªä¸ªç­”æ¡ˆæ˜¯é¦–é€‰ï¼ˆchosenï¼‰å“ªä¸ªè¢«æ‹’ç»ï¼ˆrejectedï¼‰ã€‚

**å¸¸è§é¢è¯•é¢˜ï¼š**

* SFTå’Œåå¥½æ•°æ®é›†æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
* ä¸ºä»€ä¹ˆä¼šè¯ç±»æ•°æ®æ¯”åŸå§‹æ–‡æœ¬ç¨€ç¼ºï¼Ÿ
* ä¸ºä»€ä¹ˆéœ€è¦å¯¹ç§å­æ•°æ®è¿›è¡Œå¤„ç†ï¼Ÿ

---

## äºŒã€å­˜å‚¨ä¸Chatæ¨¡æ¿ï¼ˆStorage & Chat Templatesï¼‰

**ç®€å•è§£é‡Šï¼š**

* æ•°æ®é›†é€šå¸¸ç”¨åƒShareGPTã€OpenAI/HFè¿™æ ·çš„æ ¼å¼ä¿å­˜å¯¹è¯å†…å®¹ã€‚
* è¿™äº›åŸå§‹æ ¼å¼å†æ˜ å°„åˆ°â€œchatæ¨¡æ¿â€ï¼ˆæ¯”å¦‚ChatMLã€Alpacaï¼‰ï¼Œç”¨äºæ¨¡å‹å®é™…è®­ç»ƒã€‚Chatæ¨¡æ¿å°±æ˜¯è§„èŒƒæ¯æ¡å¯¹è¯çš„è¾“å…¥è¾“å‡ºæ ¼å¼ï¼Œè®©æ¨¡å‹èƒ½â€œçœ‹æ‡‚â€æ•°æ®ã€‚

**å¸¸è§é¢è¯•é¢˜ï¼š**

* ä¸ºä»€ä¹ˆè¦ç”¨chatæ¨¡æ¿ï¼Ÿ
* ChatMLå’ŒAlpacaçš„æ ¼å¼æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
* chatæ¨¡æ¿å¦‚ä½•å½±å“æ¨¡å‹è®­ç»ƒï¼Ÿ

---

## ä¸‰ã€åˆæˆæ•°æ®ç”Ÿæˆï¼ˆSynthetic Data Generationï¼‰

**ç®€å•è§£é‡Šï¼š**

* ç”¨åƒGPT-4oè¿™æ ·çš„å¼ºå¤§æ¨¡å‹ï¼Œæ ¹æ®è®¾è®¡å¥½çš„ç§å­ä»»åŠ¡ï¼ˆæŒ‡ä»¤ï¼‰æ‰¹é‡ç”Ÿæˆä¼˜è´¨çš„æŒ‡ä»¤-ç­”æ¡ˆå¯¹ã€‚
* è¿™æ ·å¯ä»¥å¿«é€Ÿæ‰©å……æ•°æ®é›†ï¼Œå°¤å…¶æ˜¯ä¸€äº›ç½•è§é¢†åŸŸæˆ–å¤æ‚ä»»åŠ¡ã€‚

**å¸¸è§é¢è¯•é¢˜ï¼š**

* åˆæˆæ•°æ®ç›¸è¾ƒäºäººå·¥æ ‡æ³¨æ•°æ®çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
* å¦‚ä½•ç¡®ä¿åˆæˆæ•°æ®çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Ÿ
* è®¾è®¡å¥½çš„system promptå¯¹æ•°æ®ç”Ÿæˆæœ‰ä½•å½±å“ï¼Ÿ

---

## å››ã€æ•°æ®å¢å¼ºï¼ˆData Enhancementï¼‰

**ç®€å•è§£é‡Šï¼š**

ä¼˜åŒ–å’Œæ‰©å±•å·²æœ‰æ•°æ®ï¼Œå¸¸è§æ–¹æ³•æœ‰ï¼š

* **Verified outputs** ï¼šé’ˆå¯¹ä»£ç ç±»é—®é¢˜ï¼Œç”¨å•å…ƒæµ‹è¯•/éªŒè¯å™¨è‡ªåŠ¨æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚
* **å¤šç­”æ¡ˆ+æ‹’ç»é‡‡æ ·ï¼ˆrejection samplingï¼‰** ï¼šè®©æ¨¡å‹ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼Œåªä¿ç•™æœ€ä¼˜ç­”æ¡ˆã€‚
* **Auto-Evol** ï¼šè‡ªåŠ¨è®©æŒ‡ä»¤å˜å¾—æ›´å¤æ‚ï¼ˆè¿›åŒ–ï¼‰ã€‚
* **Chain-of-Thought** ï¼šè®©æ¨¡å‹è¾“å‡ºæ¨ç†æ­¥éª¤ï¼Œè€Œä¸ä»…æ˜¯æœ€ç»ˆç­”æ¡ˆã€‚
* **Branch-Solve-Merge** ï¼šæ‹†è§£å¤æ‚é—®é¢˜ï¼Œåˆ†åˆ«è§£å†³ååˆå¹¶ã€‚
* **Personas** ï¼šè®©æ¨¡å‹æ¨¡æ‹Ÿä¸åŒèº«ä»½ï¼ˆå¦‚è€å¸ˆã€åŒ»ç”Ÿï¼‰æ¥ä½œç­”ã€‚

**å¸¸è§é¢è¯•é¢˜ï¼š**

* Chain-of-Thoughtï¼ˆCoTï¼‰æ˜¯ä»€ä¹ˆï¼Ÿå¯¹æ¨¡å‹è®­ç»ƒæœ‰ä»€ä¹ˆå¸®åŠ©ï¼Ÿ
* ä»€ä¹ˆæ˜¯rejection samplingï¼Ÿå¦‚ä½•åœ¨æ•°æ®å¢å¼ºä¸­ä½¿ç”¨ï¼Ÿ
* Personasèƒ½å¸¦æ¥å“ªäº›å¥½å¤„ï¼Ÿ

---

## äº”ã€è´¨é‡è¿‡æ»¤ï¼ˆQuality Filteringï¼‰

**ç®€å•è§£é‡Šï¼š**

ç¡®ä¿æ•°æ®å¹²å‡€å’Œé«˜è´¨é‡ï¼Œå¸¸ç”¨æ‰‹æ®µæœ‰ï¼š

* **è§„åˆ™è¿‡æ»¤** ï¼šå»é™¤è„è¯ã€æ— æ„ä¹‰å†…å®¹ã€è¿‡çŸ­æˆ–è¿‡é•¿çš„æ ·æœ¬ç­‰ã€‚
* **å»é‡** ï¼šç”¨MinHashæˆ–å‘é‡ç›¸ä¼¼åº¦å»é™¤é‡å¤/è¿‘é‡å¤æ ·æœ¬ï¼Œé˜²æ¨¡å‹è®°å¿†ã€‚
* **n-gramå»æ±¡æŸ“ï¼ˆdecontaminationï¼‰** ï¼šé¿å…è®­ç»ƒé›†å’Œè¯„æµ‹é›†æœ‰é‡å ï¼Œä¿è¯è¯„æµ‹ç»“æœçœŸå®ã€‚
* **å¥–åŠ±æ¨¡å‹ï¼ˆReward Modelsï¼‰ä¸Judge LLMs** ï¼šç”¨æ¨¡å‹è‡ªåŠ¨åˆ¤åˆ†ã€æ‰“åˆ†è¿‡æ»¤ä½è´¨é‡æ ·æœ¬ï¼Œå®ç°æ›´ç»†è‡´çš„è´¨é‡æ§åˆ¶ã€‚

**å¸¸è§é¢è¯•é¢˜ï¼š**

* ä¸ºä»€ä¹ˆè¦å»é‡ï¼Ÿæœ‰å“ªäº›å»é‡æ–¹æ³•ï¼Ÿ
* ä»€ä¹ˆæ˜¯n-gramå»æ±¡æŸ“ï¼Ÿä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ
* å¥–åŠ±æ¨¡å‹å’ŒJudge LLMèµ·åˆ°ä»€ä¹ˆä½œç”¨ï¼Ÿ

---

**æ€»ç»“ä¸€å¥ï¼š**

Post-trainingçš„æ•°æ®é›†ç»“æ„å’Œå¤„ç†å…³æ³¨â€œå¦‚ä½•è®©LLMå­¦åˆ°æ›´åƒäººçš„ã€å¯¹è¯å¼ã€ä¸°å¯Œä¸”é«˜è´¨é‡çš„çŸ¥è¯†â€ï¼Œæ¶‰åŠæ ¼å¼è§„èŒƒã€æ™ºèƒ½æ•°æ®ç”Ÿæˆã€é«˜çº§æ•°æ®ä¼˜åŒ–å’Œå¤šå±‚æ¬¡è´¨é‡æ§åˆ¶ï¼Œæ˜¯LLMè®­ç»ƒç¯èŠ‚ä¸­è‡³å…³é‡è¦çš„ä¸€ç¯ã€‚

å¦‚æœä½ å‡†å¤‡ç›¸å…³é¢è¯•ï¼Œå¯ä»¥æ ¹æ®æ¯ä¸ªè¦ç‚¹å¤šæŸ¥äº›å®é™…æ¡ˆä¾‹æˆ–æ·±æŒ–å…¸å‹æµç¨‹ï¼Œä¼šæ›´æœ‰å¸®åŠ©ï¼
